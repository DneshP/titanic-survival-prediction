{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The goal is to predict which passengers survived the Titanic shipwreck.\n\n## Variable Notes\n\n- **pclass:** A proxy for socio-economic status (SES)\n  - 1st = Upper\n  - 2nd = Middle\n  - 3rd = Lower\n\n- **age:** Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5.\n\n- **sibsp:** The dataset defines family relations in this way...\n  - Sibling = brother, sister, stepbrother, stepsister\n  - Spouse = husband, wife (mistresses and fianc√©s were ignored)\n\n- **parch:** The dataset defines family relations in this way...\n  - Parent = mother, father\n  - Child = daughter, son, stepdaughter, stepson\n  - Some children traveled only with a nanny, therefore parch=0 for them.\n","metadata":{}},{"cell_type":"code","source":"# Let's take a look at the data\n\nimport pandas as pd\n\nfile_path = '/kaggle/input/titanic/train.csv'\n\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv(file_path)\n\n# Display the first few rows of the DataFrame to inspect the data\nprint(df.head(10))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.434785Z","iopub.execute_input":"2023-11-19T08:22:09.435274Z","iopub.status.idle":"2023-11-19T08:22:09.453100Z","shell.execute_reply.started":"2023-11-19T08:22:09.435234Z","shell.execute_reply":"2023-11-19T08:22:09.452078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n# identify missing values\nmissing_column_values_df = df.loc[:, df.isnull().any()]\nprint(missing_column_values_df.columns, \"\\n\", missing_column_values_df.dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.455768Z","iopub.execute_input":"2023-11-19T08:22:09.456068Z","iopub.status.idle":"2023-11-19T08:22:09.471823Z","shell.execute_reply.started":"2023-11-19T08:22:09.456042Z","shell.execute_reply":"2023-11-19T08:22:09.470847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of male and female passengers\nmale_percentage = (df['Sex'] == 'male').sum() / len(df) * 100\nfemale_percentage = (df['Sex'] == 'female').sum() / len(df) * 100\n\n# Display the percentages\nprint(f\"Percentage of male passengers: {male_percentage:.2f}%\")\nprint(f\"Percentage of female passengers: {female_percentage:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.485358Z","iopub.execute_input":"2023-11-19T08:22:09.485985Z","iopub.status.idle":"2023-11-19T08:22:09.492258Z","shell.execute_reply.started":"2023-11-19T08:22:09.485932Z","shell.execute_reply":"2023-11-19T08:22:09.491530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset\nfrom sklearn.model_selection import train_test_split\n\n# Define features (X) and target variable (y)\nX = df.drop('Survived', axis=1)\ny = df['Survived']\n\n# Split the data into training and testing sets with stratification\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['Sex'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.512934Z","iopub.execute_input":"2023-11-19T08:22:09.513686Z","iopub.status.idle":"2023-11-19T08:22:09.523274Z","shell.execute_reply.started":"2023-11-19T08:22:09.513651Z","shell.execute_reply":"2023-11-19T08:22:09.522048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of male and female passengers\nmale_percentage_train = (X_train['Sex'] == 'male').sum() / len(X_train) * 100\nfemale_percentage_train = (X_train['Sex'] == 'female').sum() / len(X_train) * 100\n\nmale_percentage_test = (X_train['Sex'] == 'male').sum() / len(X_train) * 100\nfemale_percentage_test = (X_train['Sex'] == 'female').sum() / len(X_train) * 100\n\n# Display the percentages\nprint(f\"Percentage of male passengers train: {male_percentage_train:.2f}%\")\nprint(f\"Percentage of female passengers train: {female_percentage_train:.2f}%\")\nprint(f\"Percentage of male passengers test: {male_percentage_test:.2f}%\")\nprint(f\"Percentage of female passengers test: {female_percentage_test:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.533113Z","iopub.execute_input":"2023-11-19T08:22:09.534663Z","iopub.status.idle":"2023-11-19T08:22:09.544891Z","shell.execute_reply.started":"2023-11-19T08:22:09.534618Z","shell.execute_reply":"2023-11-19T08:22:09.543623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocesing pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n# Define the features and target column\nfeatures = np.setdiff1d(df.columns, ['Survived'])\ntarget = 'Survived'\n\nnum_features = make_column_selector(dtype_include=\"number\")(df[features])\ncat_features = make_column_selector(dtype_include=object)(df[features])\n\n# Custom transformer to drop specified columns\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_to_drop):\n        self.columns_to_drop = columns_to_drop\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if not set(self.columns_to_drop).issubset(X.columns):\n            missing_columns = list(set(self.columns_to_drop) - set(X.columns))\n            raise ValueError(f\"Columns {missing_columns} not found in the DataFrame\")\n\n        # Drop the specified columns\n        X_transformed = X.drop(columns=self.columns_to_drop)\n        return X_transformed\n\n    def get_feature_names_out(self, input_features=None):\n        # Exclude the dropped columns from the input feature names\n        return [col for col in input_features if col not in self.columns_to_drop]\n\n# Custom transformer to combine two columns\nclass ColumnCombiner(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_to_combine, new_column_name):\n        self.columns_to_combine = columns_to_combine\n        self.new_column_name = new_column_name\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Combine specified columns and create a new one\n        X[self.new_column_name] = X[self.columns_to_combine[0]] + X[self.columns_to_combine[1]]\n        return X\n\n    def get_feature_names_out(self, input_features=None):\n        # Return the input feature names and the name of the new column\n        return input_features + [self.new_column_name]\n\n\n# Custom transformer to apply log\nclass LogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns_to_log):\n        self.columns_to_log = columns_to_log\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Convert X to DataFrame if it's not already\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X, columns=self.columns_to_log)\n\n        X_copy = X.copy()\n        # Apply log transformation to specified columns\n        for column in self.columns_to_log:\n            X_copy[column] = np.log1p(X_copy[column])\n\n        return X_copy\n\n    def get_feature_names_out(self, input_features=None):\n        # Return the input feature names and the names of the log-transformed columns\n        return input_features + [f\"{column}_log\" for column in self.columns_to_log]\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X.toarray() if hasattr(X, 'toarray') else X\n\n    def get_feature_names_out(self, input_features=None):\n        return input_features\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"log_transform\", LogTransformer(columns_to_log=num_features)),\n    (\"standardize\", StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"one_hot_encode\", OneHotEncoder(handle_unknown=\"ignore\")),\n    (\"to_dense\", DenseTransformer())\n])\npreprocessing = make_column_transformer(\n    (ColumnDropper(columns_to_drop=['Name', \"Ticket\"]), ['Name', \"Ticket\"]),\n    (ColumnCombiner(columns_to_combine=['SibSp', 'Parch'], new_column_name='Dependents'), ['SibSp', 'Parch']),\n    (num_pipeline, num_features),\n    (cat_pipeline, cat_features),\n    remainder='passthrough'\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:43:54.692003Z","iopub.execute_input":"2023-11-19T08:43:54.692413Z","iopub.status.idle":"2023-11-19T08:43:54.715375Z","shell.execute_reply.started":"2023-11-19T08:43:54.692377Z","shell.execute_reply":"2023-11-19T08:43:54.714161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed = preprocessing.fit_transform(X_train)\nprint(df_processed.shape)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:43:56.780496Z","iopub.execute_input":"2023-11-19T08:43:56.780910Z","iopub.status.idle":"2023-11-19T08:43:56.817705Z","shell.execute_reply.started":"2023-11-19T08:43:56.780877Z","shell.execute_reply":"2023-11-19T08:43:56.816682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pipeline\npreprocessing","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:35:39.359388Z","iopub.execute_input":"2023-11-19T08:35:39.359785Z","iopub.status.idle":"2023-11-19T08:35:39.415992Z","shell.execute_reply.started":"2023-11-19T08:35:39.359751Z","shell.execute_reply":"2023-11-19T08:35:39.414772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Summary\n\nThe correlation matrix provides insights into the relationships between different features and the target variable 'Survived'. Here are some key correlations:\n\n### Positive Correlations with Survival:\n\n- Being Female (pipeline-2__Sex_female): 0.692025\n  - This suggests a strong positive correlation between being female and survival. Female passengers were more likely to survive.\n\n- Higher Fare (pipeline-1__Fare): 0.414994\n  - Passengers who paid higher fares had a positive correlation with survival, indicating a potential association between fare and survival.\n\n- Embarked at Cherbourg (pipeline-2__Embarked_C): 0.286461\n  - Passengers who embarked at Cherbourg had a positive correlation with survival.\n\n### Negative Correlations with Survival:\n\n- Lower Passenger Class (pipeline-1__Pclass): -0.472895\n  - There is a negative correlation with passenger class, indicating that lower-class passengers were less likely to survive.\n\n- Being Male (pipeline-2__Sex_male): -0.692025\n  - This strong negative correlation suggests that being male is associated with a lower likelihood of survival.\n\n### Other Correlations:\n\n- Parch (pipeline-1__Parch): 0.163889\n  - A positive correlation with survival, but not as strong as being female or having a higher fare.\n\n- Embarked at Southampton (pipeline-2__Embarked_S): -0.244726\n  - Negative correlation with survival. Passengers who embarked at Southampton had a lower chance of survival.\n\n- Cabin B96 B98 (pipeline-2__Cabin_B96 B98): -0.460145\n  - Negative correlation with survival. Passengers with this cabin had a lower likelihood of survival.\n\n- Ticket 3101295 (pipeline-2__Ticket_3101295): -0.093152\n  - Negative correlation with survival, but not as strong.\n\nThese correlations provide valuable insights into the factors influencing survival on the Titanic.\n","metadata":{}},{"cell_type":"code","source":"# need to fix get feature names out \n\n# Convert the sparse matrix to a dense Pandas DataFrame with correct column names\ndf_processed_dense = pd.DataFrame(df_processed)\n\n# # Add the 'Survived' column to the processed DataFrame\ndf_processed_dense['Survived'] = df['Survived']\n\n# # Compute the correlation matrix for the processed data\ncorr_matrix_processed = df_processed_dense.corr()\n\n# # Coorelation\ncorr_matrix = corr_matrix_processed.corr(numeric_only=True)\ncorr_matrix[\"Survived\"].sort_values(ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:58:36.317385Z","iopub.execute_input":"2023-11-19T08:58:36.317779Z","iopub.status.idle":"2023-11-19T08:58:47.918696Z","shell.execute_reply.started":"2023-11-19T08:58:36.317750Z","shell.execute_reply":"2023-11-19T08:58:47.917574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Define the hyperparameter grid\nparam_grid = {\n#     'classifier__criterion': ['gini', 'entropy'],\n    'classifier__criterion': ['entropy'],\n#     'classifier__splitter': ['best', 'random'],\n        'classifier__splitter': ['best'],\n#     'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n    'classifier__max_depth': [30,],\n#     'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_split': [5,],\n#     'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__min_samples_leaf': [1,],\n#     'classifier__max_features': ['auto', 'sqrt', 'log2', None],\n    'classifier__max_features': [None],\n    'classifier__ccp_alpha': [0.0, 0.01, 0.1, 0.2, 0.5, 1.0]\n}\n\n# Decision tree pipeline with RandomizedSearchCV\ntree_model = Pipeline([\n    (\"preprocessing\", preprocessing),\n    (\"classifier\", DecisionTreeClassifier(random_state=42))\n])\n\n# Set up RandomizedSearchCV on the pipeline\nrandom_search = RandomizedSearchCV(tree_model, param_distributions=param_grid, n_iter=100, scoring='accuracy', cv=5, n_jobs=-1, random_state=42)\n\n# Perform the search on the training data\nrandom_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = random_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Get the best model\nbest_tree_model = random_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:59:06.864734Z","iopub.execute_input":"2023-11-19T08:59:06.865222Z","iopub.status.idle":"2023-11-19T08:59:09.850405Z","shell.execute_reply.started":"2023-11-19T08:59:06.865183Z","shell.execute_reply":"2023-11-19T08:59:09.849428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best Hyperparameters: {'classifier__splitter': 'best', 'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 1, 'classifier__max_features': None, 'classifier__max_depth': 30, 'classifier__criterion': 'entropy'}","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n\n# Evaluate the best model on the test set\ny_pred = best_tree_model.predict(X_test)\n\n# Evaluate the Decision Tree model\naccuracy_tree = accuracy_score(y_test, y_pred)\nprint(f'Best Decision Tree Accuracy: {accuracy_tree:.2f}')\n\n# Display additional evaluation metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Display the confusion matrix\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Additional metrics\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Display additional metrics\nprint(\"\\nPrecision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:59:09.852375Z","iopub.execute_input":"2023-11-19T08:59:09.853272Z","iopub.status.idle":"2023-11-19T08:59:09.896901Z","shell.execute_reply.started":"2023-11-19T08:59:09.853227Z","shell.execute_reply":"2023-11-19T08:59:09.895435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the data\n\nimport pandas as pd\n\nfile_path = '/kaggle/input/titanic/test.csv'\n\n# Load the CSV file into a Pandas DataFrame\ntest_df = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:59:12.534284Z","iopub.execute_input":"2023-11-19T08:59:12.534698Z","iopub.status.idle":"2023-11-19T08:59:12.547816Z","shell.execute_reply.started":"2023-11-19T08:59:12.534664Z","shell.execute_reply":"2023-11-19T08:59:12.546807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:22:09.923619Z","iopub.status.idle":"2023-11-19T08:22:09.924407Z","shell.execute_reply.started":"2023-11-19T08:22:09.924179Z","shell.execute_reply":"2023-11-19T08:22:09.924202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the best model on the test set\ntest_predictions = best_tree_model.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:59:13.806411Z","iopub.execute_input":"2023-11-19T08:59:13.806819Z","iopub.status.idle":"2023-11-19T08:59:13.835619Z","shell.execute_reply.started":"2023-11-19T08:59:13.806786Z","shell.execute_reply":"2023-11-19T08:59:13.834226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame for the submission\nsubmission_df = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': test_predictions\n})\n\n# Save the DataFrame to a CSV file\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:59:16.016294Z","iopub.execute_input":"2023-11-19T08:59:16.016707Z","iopub.status.idle":"2023-11-19T08:59:16.028325Z","shell.execute_reply.started":"2023-11-19T08:59:16.016676Z","shell.execute_reply":"2023-11-19T08:59:16.027187Z"},"trusted":true},"execution_count":null,"outputs":[]}]}